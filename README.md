[![Badge License]][license]

<a name="readme-top"></a>

<div align = center>

# Linear Quadratic Regulator (LQR) and Reinforcement Learning

_This repository contains two laboratory exercises focused on understanding optimal control theory, specifically the Linear Quadratic Regulator (LQR) and integrating the Proximal Policy Optimization (PPO) reinforcement learning algorithm_

## Team

[![Badge Marco]][marco]
[![Badge Andrea]][andrea]


<br>


<div align = left>

<!-- TABLE OF CONTENTS -->
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href="#linear-quadratic-regulator-lqr">Linear Quadratic Regulator (LQR)</a></li>
    <li><a href="#reinforcement-learning-through-ppo">Reinforcement Learning through PPO</a></li>
    <li><a href="#requirements">Requirements</a></li>
    <li><a href="#license">License</a></li>
    <li><a href="#contact">Contact</a></li>
  </ol>
</details>


## Linear Quadratic Regulator (LQR)
<a href=https://github.com/marco-milanesi/lqr-ppo/blob/main/Source/Forearm%20Movements%20LQR%20Model.ipynb>`./Source/Forearm Movements LQR Model.ipynb`</a> introduces LQR by first performing basic manipulations of the data and then manually applying the LQR.
I The data used in this laboratory exercise come from a 2011 experiment where participants aimed towards a thin line using 5 different strategies ranging from fast to precise. The data can be found in the `Dataset` folder.

The lab has 4 parts: the first part is about basic manipulations of the data, and visualizing that data. The second and third part are ”courses”, and we will treat them together. The fourth part is about applying what you just learned in the course.

<p align="right">(<a href="#readme-top">back to top</a>)</p>


## Reinforcement Learning through PPO

<a href=https://github.com/marco-milanesi/lqr-ppo/blob/main/Source/Reinforcement%20Learning%20through%20PPO.ipynb>`./Source/Reinforcement Learning through PPO.ipynb`</a>  focuses on using the model-free reinforcement learning algorithm, PPO, to solve the optimization problem. The laboratory exercise uses the stable-baselines3 library, which contains an implementation of PPO. 

<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * * 
<div align = center>

_The results of the two laboratory exercises are represented and commented on within the notebook._

<div align = left>

## Requirements 
Both laboratory exercises were developed in Google Colab and the required installations are detailed within each notebook.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

<!-- LICENSE -->
## License

Distributed under the MIT License. See <a href=https://github.com/marco-milanesi/lqr-ppo/blob/main/LICENSE>`LICENSE`</a>  for more information.

<p align="right">(<a href="#readme-top">back to top</a>)</p>



<!-- CONTACT -->
## Contact

- Marco Milanesi - <a href = "mailto: marco.milanesi.99@gmail.com">marco.milanesi.99@gmail.com</a>
- Andrea Campanelli - <a href = "mailto: a.campaneli@studenti.unibs.it">a.campanelli@studenti.unibs.it</a>


<p align="right">(<a href="#readme-top">back to top</a>)</p>

<!----------------------------------------------------------------------------->

[marco]: https://github.com/marco-milanesi
[andrea]: https://github.com/gianandry

[license]: LICENSE

<!---------------------------------{ Badges }---------------------------------->

[badge license]: https://img.shields.io/badge/License-MIT-yellow.svg?style=for-the-badge
[badge marco]: https://img.shields.io/badge/Marco_Milanesi-4776c1?style=for-the-badge
[badge andrea]: https://img.shields.io/badge/Andrea_Campanelli-4776c1?style=for-the-badge